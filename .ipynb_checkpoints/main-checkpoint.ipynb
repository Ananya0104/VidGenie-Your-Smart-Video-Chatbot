{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01779a33-6d03-4e72-b787-40c56346427f",
   "metadata": {},
   "source": [
    "## NEW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4389263-8e87-4d9e-aa39-81ef8e46da8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import ffmpeg\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "from webvtt import WebVTT, Caption\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5a07c-54a9-4523-a9ef-e701e90320a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDownloader:\n",
    "    def __init__(self, base_dir: str = \"./shared_data/videos\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def download_video(self, url: str, video_id: str) -> Optional[Dict]:\n",
    "        output_dir = self.base_dir / video_id\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        ydl_opts = {\n",
    "            'format': 'best',\n",
    "            'outtmpl': str(output_dir / '%(title)s.%(ext)s'),\n",
    "            'quiet': True,\n",
    "            'ignoreerrors': True,\n",
    "            'writedescription': True,  # Fetch video description\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            if not info:\n",
    "                return None\n",
    "            video_path = Path(ydl.prepare_filename(info))\n",
    "            if not video_path.exists():\n",
    "                return None\n",
    "            return {\n",
    "                \"video_path\": video_path,\n",
    "                \"title\": info.get(\"title\", \"\"),\n",
    "                \"description\": info.get(\"description\", \"\"),\n",
    "                \"url\": url,\n",
    "            }\n",
    "\n",
    "    def download_subtitles(self, url: str, video_id: str) -> Optional[Path]:\n",
    "        output_dir = self.base_dir / video_id\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        ydl_opts = {\n",
    "            'skip_download': True,\n",
    "            'writesubtitles': True,\n",
    "            'writeautomaticsub': True,\n",
    "            'subtitleslangs': ['en'],\n",
    "            'subtitlesformat': 'vtt',\n",
    "            'outtmpl': str(output_dir / '%(title)s.%(ext)s'),\n",
    "            'quiet': True,\n",
    "            'ignoreerrors': True\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            if not info:\n",
    "                return None\n",
    "            subs = info.get('requested_subtitles', {}).get('en', {})\n",
    "            if subs and (sub_path := Path(subs.get('filepath', ''))).exists():\n",
    "                return sub_path\n",
    "            for f in output_dir.glob(\"*.en.vtt\"):\n",
    "                return f\n",
    "            return None\n",
    "\n",
    "\n",
    "class TranscriptProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = whisper.load_model(\"base\")\n",
    "\n",
    "    def process_subtitles(self, video_path: Path, sub_path: Optional[Path]) -> Path:\n",
    "        video_dir = video_path.parent\n",
    "        if sub_path and sub_path.exists() and sub_path.stat().st_size > 0:\n",
    "            return sub_path\n",
    "        return self._generate_subtitles(video_path, video_dir)\n",
    "\n",
    "    def _generate_subtitles(self, video_path: Path, output_dir: Path) -> Path:\n",
    "        audio_path = output_dir / \"temp_audio.wav\"\n",
    "        vtt_path = output_dir / \"generated_subtitles.vtt\"\n",
    "        (\n",
    "            ffmpeg.input(str(video_path))\n",
    "            .output(str(audio_path), acodec='pcm_s16le', ar=16000, ac=1)\n",
    "            .run(quiet=True)\n",
    "        )\n",
    "        result = self.model.transcribe(str(audio_path), task=\"translate\", language=\"en\", fp16=False)\n",
    "        self._create_vtt(result[\"segments\"], vtt_path)\n",
    "        if audio_path.exists():\n",
    "            audio_path.unlink()\n",
    "        return vtt_path\n",
    "\n",
    "    def _create_vtt(self, segments: List[Dict], output_path: Path):\n",
    "        vtt = WebVTT()\n",
    "        for seg in segments:\n",
    "            caption = Caption(\n",
    "                self._format_time(seg['start']),\n",
    "                self._format_time(seg['end']),\n",
    "                seg['text'].strip()\n",
    "            )\n",
    "            vtt.captions.append(caption)\n",
    "        vtt.save(str(output_path))\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_time(seconds: float) -> str:\n",
    "        hours = int(seconds // 3600)\n",
    "        mins = int((seconds % 3600) // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{hours:02}:{mins:02}:{secs:06.3f}\"\n",
    "\n",
    "\n",
    "class FrameProcessor:\n",
    "    def __init__(self):\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    def process_video(self, video_path: Path, vtt_path: Path) -> List[Dict]:\n",
    "        video_dir = video_path.parent\n",
    "        frames_dir = video_dir / \"frames\"\n",
    "        frames_dir.mkdir(exist_ok=True)\n",
    "        subtitles = self._parse_vtt(vtt_path)\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_interval = int(round(fps))\n",
    "        metadata = []\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % frame_interval == 0:\n",
    "                timestamp = frame_count / fps\n",
    "                frame_description = self._generate_frame_description(frame)\n",
    "                self._process_frame(frame, frame_count, timestamp, frames_dir, subtitles, metadata, frame_description)\n",
    "            frame_count += 1\n",
    "        cap.release()\n",
    "        return metadata\n",
    "\n",
    "    def _generate_frame_description(self, frame) -> str:\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        inputs = self.processor(pil_image, return_tensors=\"pt\")\n",
    "        out = self.model.generate(**inputs)\n",
    "        return self.processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    def _process_frame(self, frame, count: int, timestamp: float, frames_dir: Path, subtitles: List[Dict], metadata: List, frame_description: str):\n",
    "        frame_path = frames_dir / f\"frame_{count}_time_{timestamp:.2f}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "        metadata.append({\n",
    "            \"frame_path\": str(frame_path.relative_to(frames_dir.parent)),\n",
    "            \"timestamp\": round(timestamp, 2),\n",
    "            \"subtitles\": self._get_matching_subtitles(subtitles, timestamp),\n",
    "            \"frame_description\": frame_description\n",
    "        })\n",
    "\n",
    "    def _parse_vtt(self, path: Path) -> List[Dict]:\n",
    "        return [{\n",
    "            \"start\": self._time_to_sec(caption.start),\n",
    "            \"end\": self._time_to_sec(caption.end),\n",
    "            \"text\": caption.text.strip()\n",
    "        } for caption in WebVTT().read(str(path)).captions]\n",
    "\n",
    "    def _get_matching_subtitles(self, subtitles: List[Dict], timestamp: float) -> List[str]:\n",
    "        return [sub['text'] for sub in subtitles if sub['start'] <= timestamp <= sub['end']]\n",
    "\n",
    "    @staticmethod\n",
    "    def _time_to_sec(time_str: str) -> float:\n",
    "        parts = list(map(float, time_str.replace(',', '.').split(':')))\n",
    "        if len(parts) == 3:\n",
    "            return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
    "        elif len(parts) == 2:\n",
    "            return parts[0] * 60 + parts[1]\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def process_video_pipeline(url: str, video_id: str):\n",
    "    downloader = VideoDownloader()\n",
    "    transcript_processor = TranscriptProcessor()\n",
    "    frame_processor = FrameProcessor()\n",
    "\n",
    "    # Step 1: Download video and extract metadata\n",
    "    video_info = downloader.download_video(url, video_id)\n",
    "    if not video_info or not video_info[\"video_path\"].exists():\n",
    "        return\n",
    "\n",
    "    # Step 2: Download or generate subtitles\n",
    "    sub_path = downloader.download_subtitles(url, video_id)\n",
    "    vtt_path = transcript_processor.process_subtitles(video_info[\"video_path\"], sub_path)\n",
    "\n",
    "    # Step 3: Process frames and generate descriptions\n",
    "    frame_metadata = frame_processor.process_video(video_info[\"video_path\"], vtt_path)\n",
    "\n",
    "    # Step 4: Prepare final metadata\n",
    "    metadata = {\n",
    "        \"title\": video_info[\"title\"],\n",
    "        \"video_uri\": video_info[\"url\"],\n",
    "        \"description\": video_info[\"description\"],\n",
    "        \"transcript\": [{\n",
    "            \"start_time\": seg[\"start\"],\n",
    "            \"end_time\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"]\n",
    "        } for seg in frame_processor._parse_vtt(vtt_path)],\n",
    "        \"frames\": frame_metadata,\n",
    "    }\n",
    "\n",
    "    # Step 5: Save metadata\n",
    "    metadata_path = video_info[\"video_path\"].parent / \"metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_urls = [\n",
    "        \"https://www.youtube.com/watch?v=ftDsSB3F5kg\",\n",
    "        \"https://www.youtube.com/watch?v=kKFrbhZGNNI\",\n",
    "        \"https://www.youtube.com/watch?v=6qUxwZcTXHY\",\n",
    "        \"https://www.youtube.com/watch?v=MspNdsh0QcM\",\n",
    "        \"https://www.youtube.com/watch?v=Kf57KGwKa0w\"\n",
    "    ]\n",
    "    for idx, url in enumerate(video_urls, 1):\n",
    "        video_id = f\"video_{idx}\"\n",
    "        process_video_pipeline(url, video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60be29-4651-45b8-90f3-bb45fd0c452c",
   "metadata": {},
   "source": [
    "## RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4cf82-fc4e-478d-9524-8a1e376c34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MetadataChunker:\n",
    "    def __init__(self, metadata: Dict):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def chunk_metadata(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk the metadata into smaller, meaningful pieces.\n",
    "        Each chunk will include:\n",
    "        - Title\n",
    "        - Description\n",
    "        - Transcript segment (start_time, end_time, text)\n",
    "        - Frame description (if available for the segment)\n",
    "        - Video URI\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Chunk based on transcript segments\n",
    "        for transcript_segment in self.metadata[\"transcript\"]:\n",
    "            chunk = {\n",
    "                \"title\": self.metadata[\"title\"],\n",
    "                \"description\": self.metadata[\"description\"],\n",
    "                \"start_time\": transcript_segment[\"start_time\"],\n",
    "                \"end_time\": transcript_segment[\"end_time\"],\n",
    "                \"text\": transcript_segment[\"text\"],\n",
    "                \"video_uri\": self.metadata[\"video_uri\"],\n",
    "            }\n",
    "\n",
    "            # Add frame descriptions if available for the segment\n",
    "            frame_descriptions = []\n",
    "            for frame in self.metadata[\"frames\"]:\n",
    "                if transcript_segment[\"start_time\"] <= frame[\"timestamp\"] <= transcript_segment[\"end_time\"]:\n",
    "                    frame_descriptions.append(frame[\"frame_description\"])\n",
    "            chunk[\"frame_descriptions\"] = frame_descriptions\n",
    "\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[Dict]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for each chunk's text.\n",
    "        \"\"\"\n",
    "        texts = [self._prepare_text(chunk) for chunk in chunks]\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_text(chunk: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Prepare the text for embedding by combining relevant fields.\n",
    "        \"\"\"\n",
    "        text = f\"Title: {chunk['title']}\\nDescription: {chunk['description']}\\nTranscript: {chunk['text']}\"\n",
    "        if chunk[\"frame_descriptions\"]:\n",
    "            text += f\"\\nFrame Descriptions: {' '.join(chunk['frame_descriptions'])}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, db_path: str = \"./chroma_db\"):\n",
    "        self.client = chromadb.Client(Settings(persist_directory=db_path, is_persistent=True))\n",
    "        self.collection = self.client.get_or_create_collection(name=\"video_metadata\")\n",
    "\n",
    "    def store_chunks(self, chunks: List[Dict], embeddings: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Store chunks and their embeddings in the vector database.\n",
    "        \"\"\"\n",
    "        ids = [str(i) for i in range(len(chunks))]\n",
    "        documents = [self._prepare_document(chunk) for chunk in chunks]\n",
    "        metadatas = [self._prepare_metadata(chunk) for chunk in chunks]\n",
    "\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_document(chunk: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Prepare the document text for storage.\n",
    "        \"\"\"\n",
    "        return f\"Title: {chunk['title']}\\nDescription: {chunk['description']}\\nTranscript: {chunk['text']}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_metadata(chunk: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepare metadata for storage.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"start_time\": chunk[\"start_time\"],\n",
    "            \"end_time\": chunk[\"end_time\"],\n",
    "            \"video_uri\": chunk[\"video_uri\"],\n",
    "        }\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, db_path: str = \"./chroma_db\"):\n",
    "        self.client = chromadb.Client(Settings(persist_directory=db_path, is_persistent=True))\n",
    "        self.collection = self.client.get_collection(name=\"video_metadata\")\n",
    "        self.embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve the most relevant chunks based on the user's query.\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedding_generator.generate_embeddings([{\"title\": \"\", \"description\": \"\", \"text\": query, \"frame_descriptions\": []}])[0]\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "        )\n",
    "\n",
    "        # Format the results\n",
    "        retrieved_chunks = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            retrieved_chunks.append({\n",
    "                \"video_uri\": results[\"metadatas\"][0][i][\"video_uri\"],\n",
    "                \"start_time\": results[\"metadatas\"][0][i][\"start_time\"],\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "            })\n",
    "\n",
    "        return retrieved_chunks\n",
    "\n",
    "\n",
    "def process_all_metadata_for_vectordb(metadata_dir: str):\n",
    "    \"\"\"\n",
    "    Process all metadata files in the directory and store them in the vector database.\n",
    "    \"\"\"\n",
    "    metadata_dir = Path(metadata_dir)\n",
    "    vectordb = VectorDB()\n",
    "    embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "    # Iterate through all metadata files\n",
    "    for metadata_file in metadata_dir.glob(\"**/metadata.json\"):\n",
    "        print(f\"Processing {metadata_file}...\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open(metadata_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Chunk metadata\n",
    "        chunker = MetadataChunker(metadata)\n",
    "        chunks = chunker.chunk_metadata()\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = embedding_generator.generate_embeddings(chunks)\n",
    "\n",
    "        # Store in VectorDB\n",
    "        vectordb.store_chunks(chunks, embeddings)\n",
    "\n",
    "    print(f\"All metadata processed and stored in VectorDB.\")\n",
    "\n",
    "\n",
    "def query_vectordb(query: str):\n",
    "    \"\"\"\n",
    "    Query the vector database and retrieve relevant results.\n",
    "    \"\"\"\n",
    "    retriever = Retriever()\n",
    "    results = retriever.retrieve(query)\n",
    "\n",
    "    print(\"Retrieved Results:\")\n",
    "    for result in results:\n",
    "        print(f\"Video URI: {result['video_uri']}\")\n",
    "        print(f\"Start Time: {result['start_time']}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Process all metadata files and store in VectorDB\n",
    "    metadata_dir = \"./shared_data/videos\"\n",
    "    process_all_metadata_for_vectordb(metadata_dir)\n",
    "\n",
    "    # Step 2: Query the VectorDB\n",
    "    query = \"After completing any story, what is the next crucial step\"\n",
    "    query_vectordb(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1222939-0946-4db8-95ae-c9702816b975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
